version: '3.8'

# ==========================================
# 8GB / 2core 서버 최적화 설정
# 총 메모리 할당: ~5.5GB (시스템 예약 ~2.5GB)
# ==========================================

services:
  # ===================
  # 퍼블릭 레이어
  # ===================
  nginx-proxy-manager:
    image: jc21/nginx-proxy-manager:latest
    container_name: npm
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "81:81"  # Admin UI
    volumes:
      - npm_data:/data
      - npm_letsencrypt:/etc/letsencrypt
      - ./services/npm/custom/http_redirect.conf:/data/nginx/custom/http_redirect.conf
    networks:
      - frontend
      - backend
    deploy:
      resources:
        limits:
          memory: 384M
        reservations:
          memory: 192M

  # ==========
  # 데이터 레이어
  # ==========
  postgres:
    image: postgres:15-alpine
    container_name: postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    command: >
      postgres
      -c shared_buffers=256MB
      -c work_mem=16MB
      -c maintenance_work_mem=128MB
      -c effective_cache_size=512MB
      -c max_connections=100
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - db
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 1024M
        reservations:
          memory: 512M

  redis:
    image: redis:7-alpine
    container_name: redis
    restart: unless-stopped
    command: >
      redis-server
      --appendonly yes
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    networks:
      - db
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 384M
        reservations:
          memory: 128M

  # =================
  # 애플리케이션 레이어 (분리된 서비스)
  # =================

  # Service A - Backend (Spring Boot)
  service-a-backend:
    image: exit8/service-a-backend:latest
    container_name: service-a-backend
    restart: unless-stopped
    expose:
      - "8080"
    environment:
      SPRING_PROFILES_ACTIVE: docker
      JAVA_OPTS: "-Xms256m -Xmx512m -XX:+UseG1GC -XX:MaxGCPauseMillis=100"
      VAULT_URI: http://vault:8200
      VAULT_ROLE_ID: ${SERVICE_A_ROLE_ID}
      VAULT_SECRET_ID: ${SERVICE_A_SECRET_ID}
    networks:
      - backend
      - db
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      vault:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 768M
        reservations:
          memory: 384M

  # Service A - Frontend (Nginx)
  service-a-frontend:
    image: exit8/service-a-frontend:latest
    container_name: service-a-frontend
    restart: unless-stopped
    ports:
      - "3000:8080"
    volumes:
      - service-a-logs:/var/log/nginx
    networks:
      - frontend
      - backend
    depends_on:
      service-a-backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://127.0.0.1:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  # Service B - Backend (FastAPI)
  service-b-backend:
    image: exit8/service-b-backend:latest
    container_name: service-b-backend
    restart: unless-stopped
    expose:
      - "8000"
    environment:
      VAULT_URI: http://vault:8200
      VAULT_ROLE_ID: ${SERVICE_B_ROLE_ID}
      VAULT_SECRET_ID: ${SERVICE_B_SECRET_ID}
    networks:
      - backend
      - db
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      vault:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

  # Service B - Frontend (Nginx)
  service-b-frontend:
    image: exit8/service-b-frontend:latest
    container_name: service-b-frontend
    restart: unless-stopped
    ports:
      - "3002:8080"
    volumes:
      - service-b-logs:/var/log/nginx
    networks:
      - frontend
      - backend
    depends_on:
      service-b-backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://127.0.0.1:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M

  # ==============
  # 보안 레이어
  # ==============
  vault:
    image: hashicorp/vault:1.15
    container_name: vault
    restart: unless-stopped
    user: "root"
    ports:
      - "8200:8200"
    environment:
      VAULT_ADDR: http://0.0.0.0:8200
      SKIP_CHOWN: "true"
      SKIP_SETCAP: "true"
      VAULT_UNSEAL_KEY_1: ${VAULT_UNSEAL_KEY_1:-}
      VAULT_UNSEAL_KEY_2: ${VAULT_UNSEAL_KEY_2:-}
      VAULT_UNSEAL_KEY_3: ${VAULT_UNSEAL_KEY_3:-}
    volumes:
      - ./services/vault/config:/vault/config:rw
      - vault_data:/vault/data
      - ./services/vault/auto-unseal.sh:/vault/auto-unseal.sh:ro
    cap_add:
      - IPC_LOCK
    entrypoint: /bin/sh
    command: ["/vault/auto-unseal.sh"]
    networks:
      - backend
    healthcheck:
      test: ["CMD", "vault", "status", "-address=http://localhost:8200"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  # Note: Wazuh runs separately due to high resource usage
  # See: services/wazuh/docker-compose.wazuh.yml

  # ===================
  # 관측 레이어
  # ===================
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./services/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--storage.tsdb.retention.size=1GB'
      - '--web.enable-lifecycle'
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

  grafana:
    image: grafana/grafana:10.2.2
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3001:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
      GF_USERS_ALLOW_SIGN_UP: "false"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./services/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./services/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: 384M
        reservations:
          memory: 192M

  # Exporters for metrics collection
  node-exporter:
    image: prom/node-exporter:v1.7.0
    container_name: node-exporter
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: 96M
        reservations:
          memory: 48M

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: postgres-exporter
    restart: unless-stopped
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}?sslmode=disable"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - backend
      - db
    deploy:
      resources:
        limits:
          memory: 96M
        reservations:
          memory: 48M

  redis-exporter:
    image: oliver006/redis_exporter:v1.55.0
    container_name: redis-exporter
    restart: unless-stopped
    environment:
      REDIS_ADDR: redis:6379
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - backend
      - db
    deploy:
      resources:
        limits:
          memory: 96M
        reservations:
          memory: 48M

networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge
  db:
    driver: bridge

volumes:
  npm_data:
  npm_letsencrypt:
  postgres_data:
  redis_data:
  vault_data:
  prometheus_data:
  grafana_data:
  service-a-logs:
  service-b-logs:
