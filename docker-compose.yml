version: '3.8'

# ==========================================
# 4GB 서버 최적화 설정
# 총 메모리 할당: ~2.5GB (시스템 예약 ~1.5GB)
# ==========================================

services:
  # ===================
  # 퍼블릭 레이어
  # ===================
  nginx-proxy-manager:
    image: jc21/nginx-proxy-manager:latest
    container_name: npm
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "81:81"  # Admin UI
    volumes:
      - npm_data:/data
      - npm_letsencrypt:/etc/letsencrypt
    networks:
      - frontend
      - backend
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  # ==========
  # 데이터 레이어
  # ==========
  postgres:
    image: postgres:15-alpine
    container_name: postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-admin}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      POSTGRES_DB: ${POSTGRES_DB:-appdb}
    command: >
      postgres
      -c shared_buffers=128MB
      -c work_mem=8MB
      -c maintenance_work_mem=64MB
      -c effective_cache_size=256MB
      -c max_connections=50
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - backend
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-admin}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

  redis:
    image: redis:7-alpine
    container_name: redis
    restart: unless-stopped
    command: >
      redis-server
      --appendonly yes
      --maxmemory 128mb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    networks:
      - backend
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 192M
        reservations:
          memory: 64M

  # =================
  # 애플리케이션 레이어
  # =================
  backend-spring:
    build:
      context: ./services/backend-spring
      dockerfile: Dockerfile
    container_name: backend-spring
    restart: unless-stopped
    ports:
      - "8080:8080"
    environment:
      JAVA_OPTS: "-Xms128m -Xmx256m -XX:+UseG1GC -XX:MaxGCPauseMillis=100"
    networks:
      - backend
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 384M
        reservations:
          memory: 192M

  backend-python:
    build:
      context: ./services/backend-python
      dockerfile: Dockerfile
    container_name: backend-python
    restart: unless-stopped
    ports:
      - "8000:8000"
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --workers 2
    networks:
      - backend
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  frontend:
    build:
      context: ./services/frontend
      dockerfile: Dockerfile
    container_name: frontend
    restart: unless-stopped
    ports:
      - "3000:80"
    networks:
      - frontend
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 64M
        reservations:
          memory: 32M

  # ==============
  # 보안 레이어
  # ==============
  vault:
    image: hashicorp/vault:1.15
    container_name: vault
    restart: unless-stopped
    ports:
      - "8200:8200"
    environment:
      VAULT_ADDR: http://0.0.0.0:8200
      VAULT_API_ADDR: http://0.0.0.0:8200
    volumes:
      - ./services/vault/config:/vault/config:ro
      - vault_data:/vault/data
    cap_add:
      - IPC_LOCK
    command: server -config=/vault/config/vault.hcl
    networks:
      - backend
    healthcheck:
      test: ["CMD", "vault", "status", "-address=http://localhost:8200"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 192M
        reservations:
          memory: 64M

  # Note: Wazuh runs separately due to high resource usage
  # See: services/wazuh/docker-compose.wazuh.yml

  # ===================
  # 관측 레이어
  # ===================
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./services/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--storage.tsdb.retention.size=500MB'
      - '--web.enable-lifecycle'
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M

  grafana:
    image: grafana/grafana:10.2.2
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3001:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./services/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./services/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: 192M
        reservations:
          memory: 96M

  # Exporters for metrics collection
  node-exporter:
    image: prom/node-exporter:v1.7.0
    container_name: node-exporter
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: 64M
        reservations:
          memory: 32M

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: postgres-exporter
    restart: unless-stopped
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-changeme}@postgres:5432/${POSTGRES_DB:-appdb}?sslmode=disable"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: 64M
        reservations:
          memory: 32M

  redis-exporter:
    image: oliver006/redis_exporter:v1.55.0
    container_name: redis-exporter
    restart: unless-stopped
    environment:
      REDIS_ADDR: redis:6379
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - backend
    deploy:
      resources:
        limits:
          memory: 64M
        reservations:
          memory: 32M

networks:
  frontend:
    driver: bridge
  backend:
    driver: bridge

volumes:
  npm_data:
  npm_letsencrypt:
  postgres_data:
  redis_data:
  vault_data:
  prometheus_data:
  grafana_data:
